{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0O7aS8S2HZ/lwQRbM0jIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanaad-lims/AIRL-Internship-Questions/blob/main/q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages in Colab-friendly way\n",
        "!pip install -q --upgrade pip setuptools wheel\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q segment-anything timm matplotlib opencv-python pillow requests einops shapely yacs ftfy prettytable\n"
      ],
      "metadata": {
        "id": "dFvfoeiXhB2f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Environments. This will take a few minutes. Please be patient ;)\n",
        "! nvidia-smi\n",
        "! git clone https://github.com/microsoft/GLIP.git\n",
        "%cd GLIP\n",
        "! git checkout c663d9db8a503e04c6b76cd2e14152bab775d28a\n",
        "! pip install torch torchvision torchaudio\n",
        "! pip install einops shapely timm yacs tensorboardX ftfy prettytable pymongo\n",
        "! pip install transformers\n",
        "! python setup.py build develop --user\n",
        "! mkdir MODEL"
      ],
      "metadata": {
        "id": "lZTwMVbQhY_1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add GLIP repo to Python path\n",
        "sys.path.append(\"/content/GLIP\")\n",
        "\n",
        "# SAM imports\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# GLIP imports\n",
        "from maskrcnn_benchmark.config import cfg\n",
        "from maskrcnn_benchmark.engine.predictor_glip import GLIPDemo\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "EzLsX598hZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_image(url):\n",
        "    \"\"\"\n",
        "    Given an URL of an image, download it and return a numpy array in BGR format\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    image = np.array(pil_image)[:, :, [2,1,0]]  # RGB -> BGR\n",
        "    return image\n",
        "\n",
        "def imshow(img, caption=\"\"):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.imshow(img[:, :, [2,1,0]])  # BGR -> RGB for display\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(caption)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jiGRG5TihZKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLIP config\n",
        "config_file = \"GLIP/configs/pretrain/glip_Swin_T_O365_GoldG.yaml\"\n",
        "weight_file = \"GLIP/MODEL/glip_tiny_model_o365_goldg_cc_sbu.pth\"\n",
        "\n",
        "cfg.local_rank = 0\n",
        "cfg.num_gpus = 1\n",
        "cfg.merge_from_file(config_file)\n",
        "cfg.merge_from_list([\"MODEL.WEIGHT\", weight_file])\n",
        "cfg.merge_from_list([\"MODEL.DEVICE\", device])\n",
        "\n",
        "glip_demo = GLIPDemo(cfg, min_image_size=800, confidence_threshold=0.7, show_mask_heatmaps=False)\n"
      ],
      "metadata": {
        "id": "dVAhzZgrhl0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SAM checkpoint if not exists\n",
        "!mkdir -p /content/checkpoints\n",
        "sam_checkpoint = \"/content/checkpoints/sam_vit_h_4b8939.pth\"\n",
        "\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O {sam_checkpoint}\n",
        "\n",
        "# Load SAM model\n",
        "sam_model = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "sam_model.to(device=device)\n",
        "sam_predictor = SamPredictor(sam_model)\n"
      ],
      "metadata": {
        "id": "TO_mzTgIhl24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "image_url = \"http://farm4.staticflickr.com/3693/9472793441_b7822c00de_z.jpg\"\n",
        "image = load_image(image_url)\n",
        "text_prompt = \"bobble heads on top of the shelf\"\n",
        "\n",
        "# GLIP inference\n",
        "result_image, predictions = glip_demo.run_on_web_image(image, text_prompt, 0.5)\n",
        "imshow(result_image, caption=text_prompt)\n",
        "\n",
        "# Extract bounding boxes from GLIP predictions for SAM seeds\n",
        "boxes = predictions.bbox.cpu().numpy()  # shape: [N,4]\n"
      ],
      "metadata": {
        "id": "c_0OgEa1hoWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}